{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Embeddings Analysis - Tutorial\n",
    "\n",
    "## What are Embeddings?\n",
    "\n",
    "**Embeddings** are numerical representations of text that capture semantic meaning. Instead of treating words or sentences as discrete symbols, embeddings represent them as vectors (lists of numbers) in a high-dimensional space.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Vector Representation**: Each headline becomes a list of numbers (e.g., 384 dimensions)\n",
    "2. **Semantic Similarity**: Similar meanings → similar vectors → high cosine similarity\n",
    "3. **Distance Metric**: Cosine similarity measures how similar two vectors are\n",
    "\n",
    "### Why Use Embeddings?\n",
    "\n",
    "- Capture semantic meaning (not just word overlap)\n",
    "- Enable semantic search\n",
    "- Support clustering and classification\n",
    "- Work with any text length\n",
    "\n",
    "In this notebook, we use **sentence-transformers** (based on Hugging Face) to generate embeddings for 10,000 news headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import json\n",
    "import os\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded!\n",
      "Total headlines: 10,000\n",
      "\n",
      "Category distribution:\n",
      "category\n",
      "Technology       2500\n",
      "Sports           2500\n",
      "Politics         2500\n",
      "Entertainment    2500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First 5 headlines:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cloud computing chips face global supply shortage</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hill leads Knights to fifth straight victory</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Local referendum on housing scheduled for Febr...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hill retires after 2 years with Panthers</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Allen retires after 2 years with Phoenix</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline    category\n",
       "0  cloud computing chips face global supply shortage  Technology\n",
       "1       Hill leads Knights to fifth straight victory      Sports\n",
       "2  Local referendum on housing scheduled for Febr...    Politics\n",
       "3           Hill retires after 2 years with Panthers      Sports\n",
       "4           Allen retires after 2 years with Phoenix      Sports"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load extended headlines dataset\n",
    "df = pd.read_csv('../extended/news_headlines_extended.csv')\n",
    "\n",
    "print(f\"Dataset loaded!\")\n",
    "print(f\"Total headlines: {len(df):,}\")\n",
    "print(f\"\\nCategory distribution:\")\n",
    "print(df['category'].value_counts())\n",
    "print(f\"\\nFirst 5 headlines:\")\n",
    "df[['headline', 'category']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load or Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing embeddings...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/embeddings_metadata.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading existing embeddings...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/headlines_embeddings.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/embeddings_metadata.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      9\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(embeddings)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/embeddings_metadata.json'"
     ]
    }
   ],
   "source": [
    "# Check if embeddings already exist\n",
    "embeddings_exist = os.path.exists('data/headlines_embeddings.npy')\n",
    "\n",
    "if embeddings_exist:\n",
    "    print(\"Loading existing embeddings...\")\n",
    "    embeddings = np.load('data/headlines_embeddings.npy')\n",
    "    \n",
    "    with open('data/embeddings_metadata.json', 'r', encoding='utf-8') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(embeddings):,} embeddings\")\n",
    "    print(f\"Shape: {embeddings.shape}\")\n",
    "    print(f\"Model: {metadata['model_name']}\")\n",
    "else:\n",
    "    print(\"Embeddings not found. Generating...\")\n",
    "    print(\"This will take 1-2 minutes...\\n\")\n",
    "    \n",
    "    # Load model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # Generate embeddings\n",
    "    headlines = df['headline'].tolist()\n",
    "    embeddings = model.encode(headlines, \n",
    "                             batch_size=32, \n",
    "                             show_progress_bar=True,\n",
    "                             convert_to_numpy=True)\n",
    "    \n",
    "    # Save for future use\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    np.save('data/headlines_embeddings.npy', embeddings)\n",
    "    \n",
    "    print(f\"\\nGenerated and saved {len(embeddings):,} embeddings\")\n",
    "    print(f\"Shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding Embeddings\n",
    "\n",
    "Each headline is now represented as a vector of 384 numbers. Let's explore what this means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Embedding Properties:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Number of embeddings: {embeddings.shape[0]:,}\")\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"Total parameters: {embeddings.shape[0] * embeddings.shape[1]:,}\")\n",
    "print(f\"Memory size: {embeddings.nbytes / (1024**2):.2f} MB\")\n",
    "\n",
    "print(\"\\nFirst embedding (first 10 dimensions):\")\n",
    "print(embeddings[0][:10])\n",
    "print(f\"\\nCorresponding headline: {df['headline'].iloc[0]}\")\n",
    "\n",
    "# Calculate norms\n",
    "norms = np.linalg.norm(embeddings, axis=1)\n",
    "print(f\"\\nEmbedding norms:\")\n",
    "print(f\"  Mean: {norms.mean():.4f}\")\n",
    "print(f\"  Std:  {norms.std():.4f}\")\n",
    "print(f\"  Min:  {norms.min():.4f}\")\n",
    "print(f\"  Max:  {norms.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cosine Similarity\n",
    "\n",
    "**Cosine similarity** measures the cosine of the angle between two vectors. It ranges from -1 to 1:\n",
    "- 1: Identical direction (very similar)\n",
    "- 0: Orthogonal (unrelated)\n",
    "- -1: Opposite direction (very dissimilar)\n",
    "\n",
    "Formula: `similarity = (A · B) / (||A|| × ||B||)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "# Compare a few headlines\n",
    "indices = [0, 1, 10, 100]\n",
    "\n",
    "print(\"Sample Headlines:\")\n",
    "for i in indices:\n",
    "    print(f\"  {i}: [{df['category'].iloc[i]}] {df['headline'].iloc[i]}\")\n",
    "\n",
    "print(\"\\nPairwise Cosine Similarities:\")\n",
    "print(\"\\n      \", end=\"\")\n",
    "for i in indices:\n",
    "    print(f\"{i:6d} \", end=\"\")\n",
    "print()\n",
    "\n",
    "for i in indices:\n",
    "    print(f\"{i:4d}: \", end=\"\")\n",
    "    for j in indices:\n",
    "        sim = cosine_similarity(embeddings[i], embeddings[j])\n",
    "        print(f\"{sim:6.3f} \", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Semantic Search Example\n",
    "\n",
    "Semantic search finds headlines similar in meaning, not just word overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, top_k=5):\n",
    "    if 'model' not in globals():\n",
    "        global model\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    query_embedding = model.encode(query, convert_to_numpy=True)\n",
    "    \n",
    "    similarities = []\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        sim = cosine_similarity(query_embedding, emb)\n",
    "        similarities.append((i, sim))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Try some queries\n",
    "queries = [\n",
    "    \"president announces new policy\",\n",
    "    \"team wins championship\",\n",
    "    \"new technology innovation\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    results = semantic_search(query, top_k=3)\n",
    "    \n",
    "    for rank, (idx, sim) in enumerate(results, 1):\n",
    "        cat = df['category'].iloc[idx]\n",
    "        headline = df['headline'].iloc[idx]\n",
    "        print(f\"{rank}. [Score: {sim:.4f}] [{cat}] {headline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dimensionality Reduction with PCA\n",
    "\n",
    "**PCA (Principal Component Analysis)** reduces 384 dimensions to 2 dimensions for visualization.\n",
    "\n",
    "It finds the directions of maximum variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Applying PCA...\")\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d_pca = pca.fit_transform(embeddings)\n",
    "\n",
    "print(f\"\\nExplained variance ratio:\")\n",
    "print(f\"  PC1: {pca.explained_variance_ratio_[0]:.4f}\")\n",
    "print(f\"  PC2: {pca.explained_variance_ratio_[1]:.4f}\")\n",
    "print(f\"  Total: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "categories = df['category'].unique()\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(categories)))\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    mask = df['category'] == category\n",
    "    ax.scatter(embeddings_2d_pca[mask, 0], \n",
    "              embeddings_2d_pca[mask, 1],\n",
    "              c=[colors[i]], \n",
    "              label=category,\n",
    "              alpha=0.6,\n",
    "              s=20)\n",
    "\n",
    "ax.set_xlabel('First Principal Component', fontsize=12)\n",
    "ax.set_ylabel('Second Principal Component', fontsize=12)\n",
    "ax.set_title('PCA Visualization of Headline Embeddings', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/pca_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to: visualizations/pca_visualization.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dimensionality Reduction with t-SNE\n",
    "\n",
    "**t-SNE (t-Distributed Stochastic Neighbor Embedding)** is better at preserving local structure.\n",
    "\n",
    "It's non-linear and often reveals clusters better than PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Applying t-SNE...\")\n",
    "print(\"This may take 2-3 minutes...\\n\")\n",
    "\n",
    "# Use a subset for faster computation in demo\n",
    "n_samples = 2000\n",
    "sample_indices = np.random.choice(len(embeddings), n_samples, replace=False)\n",
    "\n",
    "tsne = TSNE(n_components=2, \n",
    "            random_state=42, \n",
    "            perplexity=30,\n",
    "            n_iter=1000,\n",
    "            verbose=1)\n",
    "\n",
    "embeddings_2d_tsne = tsne.fit_transform(embeddings[sample_indices])\n",
    "\n",
    "print(f\"\\nt-SNE complete! Reduced {n_samples} embeddings from 384D to 2D\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "df_sample = df.iloc[sample_indices]\n",
    "\n",
    "for i, category in enumerate(categories):\n",
    "    mask = df_sample['category'] == category\n",
    "    ax.scatter(embeddings_2d_tsne[mask, 0], \n",
    "              embeddings_2d_tsne[mask, 1],\n",
    "              c=[colors[i]], \n",
    "              label=category,\n",
    "              alpha=0.6,\n",
    "              s=30)\n",
    "\n",
    "ax.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "ax.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "ax.set_title('t-SNE Visualization of Headline Embeddings (2000 samples)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/tsne_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to: visualizations/tsne_visualization.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. K-Means Clustering\n",
    "\n",
    "Let's cluster headlines and see if clusters correspond to categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"K-Means Clustering...\")\n",
    "\n",
    "n_clusters = 4  # Same as number of categories\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "silhouette_avg = silhouette_score(embeddings, cluster_labels)\n",
    "\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Silhouette score: {silhouette_avg:.4f}\")\n",
    "print(\"  (Range: -1 to 1, higher is better)\")\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df['cluster'] = cluster_labels\n",
    "\n",
    "print(\"\\nCluster sizes:\")\n",
    "print(df['cluster'].value_counts().sort_index())\n",
    "\n",
    "# Show category distribution per cluster\n",
    "print(\"\\nCategory distribution per cluster:\")\n",
    "print(pd.crosstab(df['cluster'], df['category'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters on PCA plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: Colored by actual category\n",
    "for i, category in enumerate(categories):\n",
    "    mask = df['category'] == category\n",
    "    axes[0].scatter(embeddings_2d_pca[mask, 0], \n",
    "                   embeddings_2d_pca[mask, 1],\n",
    "                   c=[colors[i]], \n",
    "                   label=category,\n",
    "                   alpha=0.5,\n",
    "                   s=20)\n",
    "\n",
    "axes[0].set_xlabel('PC1', fontsize=11)\n",
    "axes[0].set_ylabel('PC2', fontsize=11)\n",
    "axes[0].set_title('Actual Categories', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Colored by cluster\n",
    "cluster_colors = plt.cm.Set2(np.linspace(0, 1, n_clusters))\n",
    "for i in range(n_clusters):\n",
    "    mask = df['cluster'] == i\n",
    "    axes[1].scatter(embeddings_2d_pca[mask, 0], \n",
    "                   embeddings_2d_pca[mask, 1],\n",
    "                   c=[cluster_colors[i]], \n",
    "                   label=f'Cluster {i}',\n",
    "                   alpha=0.5,\n",
    "                   s=20)\n",
    "\n",
    "axes[1].set_xlabel('PC1', fontsize=11)\n",
    "axes[1].set_ylabel('PC2', fontsize=11)\n",
    "axes[1].set_title('K-Means Clusters', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/clustering_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot saved to: visualizations/clustering_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Within-Category vs Between-Category Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analyzing category similarities...\")\n",
    "\n",
    "category_similarities = {}\n",
    "between_category_sims = []\n",
    "\n",
    "for cat in df['category'].unique():\n",
    "    cat_indices = df[df['category'] == cat].index[:100].tolist()\n",
    "    \n",
    "    within_sims = []\n",
    "    for i in range(len(cat_indices)):\n",
    "        for j in range(i+1, min(i+20, len(cat_indices))):\n",
    "            sim = cosine_similarity(embeddings[cat_indices[i]], \n",
    "                                  embeddings[cat_indices[j]])\n",
    "            within_sims.append(sim)\n",
    "    \n",
    "    category_similarities[cat] = within_sims\n",
    "    \n",
    "    # Between-category\n",
    "    other_cats = df[df['category'] != cat].index[:50].tolist()\n",
    "    for i in cat_indices[:30]:\n",
    "        for j in other_cats[:10]:\n",
    "            sim = cosine_similarity(embeddings[i], embeddings[j])\n",
    "            between_category_sims.append(sim)\n",
    "\n",
    "print(\"\\nAverage Within-Category Similarities:\")\n",
    "for cat, sims in category_similarities.items():\n",
    "    print(f\"  {cat:15s}: {np.mean(sims):.4f} +/- {np.std(sims):.4f}\")\n",
    "\n",
    "print(f\"\\nAverage Between-Category Similarity: {np.mean(between_category_sims):.4f}\")\n",
    "print(f\"  (std: {np.std(between_category_sims):.4f})\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "data_to_plot = [sims for sims in category_similarities.values()]\n",
    "data_to_plot.append(between_category_sims)\n",
    "\n",
    "labels = list(category_similarities.keys()) + ['Between Categories']\n",
    "\n",
    "bp = ax.boxplot(data_to_plot, labels=labels, patch_artist=True)\n",
    "\n",
    "for i, box in enumerate(bp['boxes']):\n",
    "    if i < len(categories):\n",
    "        box.set_facecolor(colors[i])\n",
    "    else:\n",
    "        box.set_facecolor('gray')\n",
    "\n",
    "ax.set_ylabel('Cosine Similarity', fontsize=12)\n",
    "ax.set_title('Within-Category vs Between-Category Similarities', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/similarity_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to: visualizations/similarity_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Embeddings are powerful**: They capture semantic meaning in numbers\n",
    "2. **Cosine similarity works**: Headlines with similar meanings have high similarity scores\n",
    "3. **Dimensionality reduction helps**: PCA and t-SNE make high-dimensional data visible\n",
    "4. **Categories cluster**: Similar topics group together in embedding space\n",
    "5. **Within-category similarity**: Headlines in the same category are more similar than across categories\n",
    "\n",
    "### Practical Applications:\n",
    "\n",
    "- **Semantic Search**: Find relevant documents by meaning, not just keywords\n",
    "- **Clustering**: Automatically group similar texts\n",
    "- **Classification**: Train classifiers on embeddings\n",
    "- **Recommendation**: Find similar items\n",
    "- **Duplicate Detection**: Find near-duplicate content\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Try different models (BERT, RoBERTa, GPT)\n",
    "2. Fine-tune embeddings on domain-specific data\n",
    "3. Build a semantic search engine\n",
    "4. Train classifiers using embeddings as features\n",
    "5. Explore cross-lingual embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Further Exploration (Optional)\n",
    "\n",
    "Try these exercises:\n",
    "\n",
    "1. Find the centroid (average embedding) of each category\n",
    "2. Measure distance of each headline to its category centroid\n",
    "3. Find outliers (headlines far from their category centroid)\n",
    "4. Build a simple classifier using nearest centroid\n",
    "5. Visualize how embeddings change for similar headlines with different words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
