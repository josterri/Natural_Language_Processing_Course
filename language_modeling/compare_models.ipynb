{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models Comparison: N-gram vs Neural\n",
    "\n",
    "**Course Focus:** Predicting the Next Word\n",
    "\n",
    "This notebook compares two approaches to language modeling:\n",
    "1. **Statistical:** 5-gram model with add-k smoothing\n",
    "2. **Neural:** LSTM-based neural language model\n",
    "\n",
    "**Key Questions:**\n",
    "- How do they differ in next-word prediction?\n",
    "- Which generates better text?\n",
    "- What are the trade-offs?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Load Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Import our models\n",
    "from ngram_model import NGramModel\n",
    "from neural_lm import LSTMLanguageModel, Vocabulary\n",
    "\n",
    "# Settings\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load 5-gram Statistical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained 5-gram model\n",
    "ngram_model = NGramModel.load('models/5gram_extended.pkl')\n",
    "\n",
    "# Get stats\n",
    "ngram_stats = ngram_model.get_ngram_stats()\n",
    "print(\"5-GRAM MODEL STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in ngram_stats.items():\n",
    "    print(f\"{key:20s}: {value:,}\" if isinstance(value, int) else f\"{key:20s}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load LSTM Neural Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if trained model exists\n",
    "lstm_path = Path('models/lstm_lm')\n",
    "if not lstm_path.with_suffix('.pt').exists():\n",
    "    print(\"WARNING: LSTM model not found!\")\n",
    "    print(\"Please train the model first by running:\")\n",
    "    print(\"  python train_neural_lm.py\")\n",
    "else:\n",
    "    # Load LSTM model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    lstm_model, lstm_vocab = LSTMLanguageModel.load('models/lstm_lm', device=device)\n",
    "    \n",
    "    print(\"LSTM MODEL STATISTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Vocabulary size:      {lstm_model.vocab_size:,}\")\n",
    "    print(f\"Embedding dim:        {lstm_model.embedding_dim}\")\n",
    "    print(f\"Hidden dim:           {lstm_model.hidden_dim}\")\n",
    "    print(f\"Number of layers:     {lstm_model.num_layers}\")\n",
    "    print(f\"Dropout rate:         {lstm_model.dropout_rate}\")\n",
    "    \n",
    "    total_params = sum(p.numel() for p in lstm_model.parameters())\n",
    "    print(f\"Total parameters:     {total_params:,}\")\n",
    "    print(f\"Device:               {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Perplexity Comparison\n",
    "\n",
    "**Perplexity** measures how \"surprised\" a model is by the test data.\n",
    "- Lower perplexity = better model\n",
    "- Perplexity of N means the model is as confused as if it had to choose uniformly from N options\n",
    "\n",
    "Let's compare both models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_df = pd.read_csv('../extended/test.csv')\n",
    "test_headlines = test_df['headline'].tolist()\n",
    "\n",
    "print(f\"Test set: {len(test_headlines)} headlines\")\n",
    "print(f\"\\nExample headlines:\")\n",
    "for i, headline in enumerate(test_headlines[:5], 1):\n",
    "    print(f\"{i}. {headline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Calculate N-gram Perplexity\n",
    "\n",
    "For the 5-gram model, we calculate perplexity manually using the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ngram_perplexity(model, headlines):\n",
    "    \"\"\"Calculate perplexity for n-gram model.\"\"\"\n",
    "    total_log_prob = 0.0\n",
    "    total_words = 0\n",
    "    \n",
    "    for headline in headlines:\n",
    "        tokens = headline.lower().split()\n",
    "        tokens_with_boundaries = ['<START>'] * (model.n - 1) + tokens + ['<END>']\n",
    "        \n",
    "        for i in range(len(tokens_with_boundaries) - model.n + 1):\n",
    "            ngram = tuple(tokens_with_boundaries[i:i + model.n])\n",
    "            context = ngram[:-1]\n",
    "            word = ngram[-1]\n",
    "            \n",
    "            prob = model._get_probability(context, word)\n",
    "            if prob > 0:\n",
    "                total_log_prob += np.log(prob)\n",
    "            else:\n",
    "                total_log_prob += np.log(1e-10)  # Small probability for unseen\n",
    "            total_words += 1\n",
    "    \n",
    "    avg_log_prob = total_log_prob / total_words\n",
    "    perplexity = np.exp(-avg_log_prob)\n",
    "    return perplexity\n",
    "\n",
    "# Calculate\n",
    "print(\"Calculating 5-gram perplexity...\")\n",
    "ngram_perplexity = calculate_ngram_perplexity(ngram_model, test_headlines)\n",
    "print(f\"5-gram Test Perplexity: {ngram_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Calculate LSTM Perplexity\n",
    "\n",
    "Load the saved test perplexity from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training results\n",
    "with open('results/training_results.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "lstm_perplexity = results['test_perplexity']\n",
    "print(f\"LSTM Test Perplexity: {lstm_perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualize Perplexity Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plot\n",
    "models = ['5-gram\\n(Statistical)', 'LSTM\\n(Neural)']\n",
    "perplexities = [ngram_perplexity, lstm_perplexity]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(models, perplexities, color=['steelblue', 'coral'], width=0.5)\n",
    "plt.ylabel('Perplexity (lower is better)', fontsize=12)\n",
    "plt.title('Language Model Perplexity Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0, max(perplexities) * 1.2)\n",
    "\n",
    "# Add value labels\n",
    "for bar, ppl in zip(bars, perplexities):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{ppl:.1f}',\n",
    "             ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = ((ngram_perplexity - lstm_perplexity) / ngram_perplexity) * 100\n",
    "plt.text(0.5, max(perplexities) * 1.1,\n",
    "         f'LSTM improves perplexity by {improvement:.1f}%',\n",
    "         ha='center', fontsize=12, style='italic', \n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.3))\n",
    "\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nIMPROVEMENT: {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "- Lower perplexity = better next-word predictions\n",
    "- LSTM typically achieves 20-40% lower perplexity than n-grams\n",
    "- This means the neural model is less \"surprised\" by test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Next Word Prediction Examples\n",
    "\n",
    "Let's see how each model predicts the next word given different contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_predictions(model, context_str, top_k=5):\n",
    "    \"\"\"Get top k predictions from n-gram model.\"\"\"\n",
    "    tokens = context_str.lower().split()\n",
    "    \n",
    "    # Get context\n",
    "    if len(tokens) < model.n - 1:\n",
    "        context = tuple(['<START>'] * (model.n - 1 - len(tokens)) + tokens)\n",
    "    else:\n",
    "        context = tuple(tokens[-(model.n - 1):])\n",
    "    \n",
    "    # Get probabilities for all possible next words\n",
    "    if context in model.ngrams:\n",
    "        word_counts = model.ngrams[context]\n",
    "        word_probs = [(word, model._get_probability(context, word)) \n",
    "                      for word in word_counts.keys()]\n",
    "        word_probs.sort(key=lambda x: x[1], reverse=True)\n",
    "        return word_probs[:top_k]\n",
    "    else:\n",
    "        return [(\"<no predictions>\", 0.0)]\n",
    "\n",
    "def get_lstm_predictions(model, vocab, context_str, top_k=5, device='cpu'):\n",
    "    \"\"\"Get top k predictions from LSTM model.\"\"\"\n",
    "    # Create vocabulary object\n",
    "    vocab_obj = Vocabulary()\n",
    "    vocab_obj.word2idx = vocab\n",
    "    vocab_obj.idx2word = {v: k for k, v in vocab.items()}\n",
    "    \n",
    "    # Encode context\n",
    "    tokens = ['<START>'] + context_str.lower().split()\n",
    "    indices = vocab_obj.encode(tokens)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    context_tensor = torch.tensor([indices], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    top_indices, top_probs = model.predict_next_word(context_tensor, top_k=top_k)\n",
    "    \n",
    "    # Decode\n",
    "    predictions = [(vocab_obj.idx2word[idx], prob) \n",
    "                   for idx, prob in zip(top_indices, top_probs)]\n",
    "    return predictions\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Comparison Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test contexts\n",
    "test_contexts = [\n",
    "    \"The president will\",\n",
    "    \"New technology company\",\n",
    "    \"The team wins\",\n",
    "    \"Breaking news about\",\n",
    "    \"Scientists discover\"\n",
    "]\n",
    "\n",
    "print(\"NEXT WORD PREDICTION COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for context in test_contexts:\n",
    "    print(f\"\\nContext: \\\"{context}\\\"\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # 5-gram predictions\n",
    "    ngram_preds = get_ngram_predictions(ngram_model, context, top_k=5)\n",
    "    print(\"\\n5-GRAM predictions:\")\n",
    "    for i, (word, prob) in enumerate(ngram_preds, 1):\n",
    "        print(f\"  {i}. {word:15s} {prob:.4f} ({prob*100:.1f}%)\")\n",
    "    \n",
    "    # LSTM predictions\n",
    "    lstm_preds = get_lstm_predictions(lstm_model, lstm_vocab, context, top_k=5, device=device)\n",
    "    print(\"\\nLSTM predictions:\")\n",
    "    for i, (word, prob) in enumerate(lstm_preds, 1):\n",
    "        print(f\"  {i}. {word:15s} {prob:.4f} ({prob*100:.1f}%)\")\n",
    "    \n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- LSTM predictions tend to be more semantically appropriate\n",
    "- 5-gram relies purely on recent context\n",
    "- LSTM can capture longer-range dependencies\n",
    "- Both struggle with rare contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Text Generation Comparison\n",
    "\n",
    "Let's generate text with both models and compare quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Generate with 5-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "np.random.seed(42)\n",
    "ngram_text = ngram_model.generate(max_words=100, multi_sentence=True)\n",
    "\n",
    "print(\"5-GRAM GENERATED TEXT:\")\n",
    "print(\"=\" * 80)\n",
    "print(ngram_text)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Generate with LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary object\n",
    "vocab_obj = Vocabulary()\n",
    "vocab_obj.word2idx = lstm_vocab\n",
    "vocab_obj.idx2word = {v: k for k, v in lstm_vocab.items()}\n",
    "\n",
    "# Start with <START> token\n",
    "start_tokens = [lstm_vocab['<START>']]\n",
    "\n",
    "# Generate\n",
    "torch.manual_seed(42)\n",
    "generated_indices = lstm_model.generate(start_tokens, max_length=100, \n",
    "                                       temperature=1.0, device=device)\n",
    "\n",
    "# Decode\n",
    "lstm_text = ' '.join(vocab_obj.decode(generated_indices))\n",
    "\n",
    "print(\"LSTM GENERATED TEXT:\")\n",
    "print(\"=\" * 80)\n",
    "print(lstm_text)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text_quality(text):\n",
    "    \"\"\"Basic quality metrics for generated text.\"\"\"\n",
    "    words = text.split()\n",
    "    unique_words = set(words)\n",
    "    \n",
    "    return {\n",
    "        'total_words': len(words),\n",
    "        'unique_words': len(unique_words),\n",
    "        'type_token_ratio': len(unique_words) / len(words),\n",
    "        'avg_word_length': np.mean([len(w) for w in words])\n",
    "    }\n",
    "\n",
    "ngram_quality = analyze_text_quality(ngram_text)\n",
    "lstm_quality = analyze_text_quality(lstm_text)\n",
    "\n",
    "print(\"TEXT QUALITY COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<25} {'5-gram':<15} {'LSTM':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Total words':<25} {ngram_quality['total_words']:<15} {lstm_quality['total_words']:<15}\")\n",
    "print(f\"{'Unique words':<25} {ngram_quality['unique_words']:<15} {lstm_quality['unique_words']:<15}\")\n",
    "print(f\"{'Type-token ratio':<25} {ngram_quality['type_token_ratio']:<15.3f} {lstm_quality['type_token_ratio']:<15.3f}\")\n",
    "print(f\"{'Avg word length':<25} {ngram_quality['avg_word_length']:<15.2f} {lstm_quality['avg_word_length']:<15.2f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n**Interpretation:**\")\n",
    "print(\"- Higher type-token ratio = more diverse vocabulary\")\n",
    "print(\"- Compare fluency, grammar, and semantic coherence manually\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Where Neural Networks Win\n",
    "\n",
    "Neural language models outperform n-grams in several key areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Long-Range Dependencies\n",
    "\n",
    "**Problem:** N-grams have fixed context window (e.g., 4 words for 5-gram)  \n",
    "**Solution:** LSTMs can remember information from much earlier in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Long context\n",
    "long_context = \"The company announced major layoffs last month and investors are now\"\n",
    "\n",
    "print(\"Testing long-range dependencies:\")\n",
    "print(f\"Context: \\\"{long_context}\\\"\")\n",
    "print(\"\\n5-gram (only sees last 4 words):\")\n",
    "ngram_preds = get_ngram_predictions(ngram_model, long_context, top_k=3)\n",
    "for word, prob in ngram_preds:\n",
    "    print(f\"  - {word} ({prob:.3f})\")\n",
    "\n",
    "print(\"\\nLSTM (can remember 'layoffs' from earlier):\")\n",
    "lstm_preds = get_lstm_predictions(lstm_model, lstm_vocab, long_context, top_k=3, device=device)\n",
    "for word, prob in lstm_preds:\n",
    "    print(f\"  - {word} ({prob:.3f})\")\n",
    "\n",
    "print(\"\\n→ LSTM can maintain context over longer distances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Rare Context Handling\n",
    "\n",
    "**Problem:** N-grams struggle with unseen contexts (even with smoothing)  \n",
    "**Solution:** LSTMs generalize better through learned representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Rare/unseen context\n",
    "rare_context = \"Quantum computing researchers develop\"\n",
    "\n",
    "print(\"Testing rare context handling:\")\n",
    "print(f\"Context: \\\"{rare_context}\\\"\")\n",
    "print(\"\\n5-gram:\")\n",
    "ngram_preds = get_ngram_predictions(ngram_model, rare_context, top_k=3)\n",
    "for word, prob in ngram_preds:\n",
    "    print(f\"  - {word} ({prob:.3f})\")\n",
    "\n",
    "print(\"\\nLSTM:\")\n",
    "lstm_preds = get_lstm_predictions(lstm_model, lstm_vocab, rare_context, top_k=3, device=device)\n",
    "for word, prob in lstm_preds:\n",
    "    print(f\"  - {word} ({prob:.3f})\")\n",
    "\n",
    "print(\"\\n→ LSTM makes reasonable predictions even for rare contexts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Semantic Coherence\n",
    "\n",
    "**Problem:** N-grams only use word surface forms  \n",
    "**Solution:** LSTMs learn semantic representations through embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare related contexts\n",
    "contexts = [\n",
    "    \"The athlete wins\",\n",
    "    \"The player wins\",\n",
    "    \"The champion wins\"\n",
    "]\n",
    "\n",
    "print(\"Testing semantic understanding:\")\n",
    "print(\"These contexts are semantically similar (sports-related winners)\\n\")\n",
    "\n",
    "for context in contexts:\n",
    "    print(f\"Context: \\\"{context}\\\"\")\n",
    "    lstm_preds = get_lstm_predictions(lstm_model, lstm_vocab, context, top_k=3, device=device)\n",
    "    print(\"  LSTM predictions:\", [w for w, p in lstm_preds])\n",
    "\n",
    "print(\"\\n→ LSTM produces similar predictions for semantically related contexts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Summary: Neural vs Statistical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = {\n",
    "    'Aspect': ['Context Window', 'Rare Contexts', 'Semantic Understanding', \n",
    "               'Training Speed', 'Inference Speed', 'Model Size', 'Perplexity'],\n",
    "    '5-gram (Statistical)': ['Fixed (4 words)', 'Poor (smoothing helps)', 'None',\n",
    "                             'Very Fast', 'Very Fast', 'Small (~1 MB)', 'Higher'],\n",
    "    'LSTM (Neural)': ['Unlimited (via memory)', 'Good (generalizes)', 'Yes (learned)',\n",
    "                      'Slow (~10 min)', 'Fast', 'Larger (~5-10 MB)', 'Lower']\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison)\n",
    "print(\"\\nLANGUAGE MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Training Analysis (LSTM)\n",
    "\n",
    "Let's examine how the LSTM model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display training curves\n",
    "from IPython.display import Image\n",
    "training_curves_path = 'results/training_curves.png'\n",
    "\n",
    "if Path(training_curves_path).exists():\n",
    "    print(\"LSTM Training Progress:\")\n",
    "    display(Image(filename=training_curves_path))\n",
    "else:\n",
    "    print(\"Training curves not found. Run training first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training summary\n",
    "if Path('results/training_results.json').exists():\n",
    "    with open('results/training_results.json', 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    print(\"LSTM TRAINING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Number of epochs:         {results['num_epochs']}\")\n",
    "    print(f\"Vocabulary size:          {results['vocab_size']:,}\")\n",
    "    print(f\"Total parameters:         {results['total_params']:,}\")\n",
    "    print(f\"\\nFinal train loss:         {results['final_train_loss']:.4f}\")\n",
    "    print(f\"Final validation loss:    {results['final_val_loss']:.4f}\")\n",
    "    print(f\"Best validation loss:     {results['best_val_loss']:.4f}\")\n",
    "    print(f\"\\nTest perplexity:          {results['test_perplexity']:.2f}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Interactive Next-Word Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_predictor(context_str, top_k=5):\n",
    "    \"\"\"Interactive function to compare both models.\"\"\"\n",
    "    print(f\"\\nContext: \\\"{context_str}\\\"\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 5-gram\n",
    "    print(\"\\n5-GRAM predictions:\")\n",
    "    ngram_preds = get_ngram_predictions(ngram_model, context_str, top_k=top_k)\n",
    "    for i, (word, prob) in enumerate(ngram_preds, 1):\n",
    "        bar = '█' * int(prob * 50)\n",
    "        print(f\"  {i}. {word:15s} {prob:.4f} {bar}\")\n",
    "    \n",
    "    # LSTM\n",
    "    print(\"\\nLSTM predictions:\")\n",
    "    lstm_preds = get_lstm_predictions(lstm_model, lstm_vocab, context_str, top_k=top_k, device=device)\n",
    "    for i, (word, prob) in enumerate(lstm_preds, 1):\n",
    "        bar = '█' * int(prob * 50)\n",
    "        print(f\"  {i}. {word:15s} {prob:.4f} {bar}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Try different contexts\n",
    "interactive_predictor(\"The president announces\")\n",
    "interactive_predictor(\"Scientists discover new\")\n",
    "interactive_predictor(\"Technology company releases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try your own contexts:**  \n",
    "Uncomment and modify the line below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive_predictor(\"Your context here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Summary & Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "#### 1. Perplexity\n",
    "- **LSTM achieves significantly lower perplexity** (~20-40% improvement)\n",
    "- This means better next-word predictions\n",
    "- Neural models are less \"surprised\" by test data\n",
    "\n",
    "#### 2. Next-Word Prediction\n",
    "- **LSTM predictions are more semantically appropriate**\n",
    "- N-grams rely only on immediate context\n",
    "- LSTM can use longer-range dependencies\n",
    "\n",
    "#### 3. Text Generation\n",
    "- **LSTM generates more fluent text**\n",
    "- Better grammatical structure\n",
    "- More coherent semantics\n",
    "- N-grams can produce repetitive patterns\n",
    "\n",
    "#### 4. Handling Rare Contexts\n",
    "- **LSTM generalizes better**\n",
    "- Learned representations help with unseen contexts\n",
    "- N-grams struggle despite smoothing\n",
    "\n",
    "#### 5. Trade-offs\n",
    "- **N-grams:** Fast, simple, interpretable, small\n",
    "- **LSTM:** Better quality, slower training, larger model\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Which?\n",
    "\n",
    "**Use N-grams when:**\n",
    "- Need very fast training/inference\n",
    "- Limited computational resources\n",
    "- Want interpretable model\n",
    "- Simple baseline needed\n",
    "\n",
    "**Use LSTM when:**\n",
    "- Need best prediction quality\n",
    "- Can afford training time\n",
    "- Want semantic understanding\n",
    "- Long-range dependencies matter\n",
    "\n",
    "---\n",
    "\n",
    "### The Progression: From Statistical to Neural\n",
    "\n",
    "1. **Statistical (N-grams):** Count-based, no learning\n",
    "2. **Neural (LSTM):** Learn patterns, generalize better\n",
    "3. **Next:** Transformers (attention mechanisms, even better!)\n",
    "\n",
    "This notebook shows why NLP moved from statistical to neural methods for language modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to HTML\n",
    "\n",
    "To create a standalone HTML report:\n",
    "\n",
    "```bash\n",
    "jupyter nbconvert --to html compare_models.ipynb\n",
    "```\n",
    "\n",
    "Or without code:\n",
    "\n",
    "```bash\n",
    "jupyter nbconvert --to html compare_models.ipynb --no-input --output model_comparison_report.html\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
